<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Dan Friedman</title>
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
		<link href="style.css" rel="stylesheet">
	</head>

	<body>
		<h1 style="text-align: center">Dan Friedman</h1>
		<div class="container">
			<div style="flex-basis: 74%">
				<p>
				I am a machine learning researcher on the Apple Foundation Models team.
				I did my PhD in the Princeton Computer Science department, advised by Danqi Chen.
				<a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>.
				For the summer and fall of 2023, I was a student researcher at Google Research, where I was advised by
				<a href="https://web.media.mit.edu/~asma_gh/">Asma Ghandeharioun</a> and
				<a href="https://lampinen.github.io/">Andrew Lampinen</a>.
				Before graduate school, I was a software engineer at Google and IBM,
				and before that I was an undergraduate at Yale where I worked
				with <a href="https://bobfrank1.github.io/">Bob Frank</a> and
				<a href="http://www.cs.yale.edu/homes/radev/">Dragomir Radev</a>,
				and received a BA in English.
				</p>
				<p>
				My PhD research was partially supported by a <a href="https://research.google/programs-and-events/phd-fellowship/recipients/">Google PhD Fellowship</a> and I was named a <a href="https://www.siebelscholars.com/siebel-scholars/">2025 Siebel Scholar</a>.
				</p>
				<p style="text-align:center">
				<a href="mailto:dfriedman@princeton.edu">Email</a> &nbsp;/&nbsp;
				<a href="https://scholar.google.com/citations?user=1UMQ_KwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
				<a href="https://twitter.com/danfriedman0">Twitter</a> &nbsp;/&nbsp;
				<a href="https://github.com/danfriedman0/">Github</a>
				</p>
			</div>
			<img src="profile_square.jpg" width="200" height="200" style="flex-basis: 24%">
		</div>
		<h2>Publications and preprints</h2>
		<ul>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2601.22379">
					SPLA: Block Sparse Plus Linear Attention for Long Context Modeling 
				</a>
				<br>
				Bailin Wang, <strong>Dan Friedman</strong>, Tao Lei, Chong Wang
				<br>
				preprint
				<br>
				<a href="https://arxiv.org/abs/2601.22379">Paper</a>
			</li>
			<br>			
			<li>
				<a class="paper" href="https://www.arxiv.org/abs/2510.18148">
					Extracting Rule-based Descriptions of Attention Features in Transformers 
				</a>
				<br>
				<strong>Dan Friedman</strong>, Adithya Bhaskar, Alexander Wettig, Danqi Chen
				<br>
				preprint
				<br>
				<a href="https://www.arxiv.org/abs/2510.18148">Paper</a> | <a href="https://github.com/princeton-nlp/AttentionRules">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2407.10949">
					Representing Rule-based Chatbots with Transformers
				</a>
				<br>
				<strong>Dan Friedman</strong>, Abhishek Panigrahi, Danqi Chen
				<br>
				NAACL 2025
				<br>
				<a href="https://arxiv.org/abs/2407.10949">Paper</a> | <a href="https://github.com/princeton-nlp/ELIZA-Transformer">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2411.07175">
					Continual Memorization of Factoids in Language Models 
				</a>
				<br>
				Howard Chen, Jiayi Geng, Adithya Bhaskar, <strong>Dan Friedman</strong>, Danqi Chen
				<br>
				preprint
				<br>
				<a href="https://arxiv.org/abs/2411.07175">Paper</a> | <a href="https://github.com/princeton-nlp/continual-factoid-memorization">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2406.16778">
					Finding Transformer Circuits with Edge Pruning
				</a>
				<br>
				Adithya Bhaskar, Alexander Wettig, <strong>Dan Friedman</strong>, Danqi Chen
				<br>
				NeurIPS 2024 <strong>Spotlight</strong>
				<br>
				<a href="https://arxiv.org/abs/2406.16778">Paper</a> | <a href="https://github.com/princeton-nlp/Edge-Pruning">Code</a>
			</li>	    
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2309.13638">
          Embers of autoregression show how large language models are shaped by the problem they are trained to solve
				</a>
				<br>
				R. Thomas McCoy, Shunyu Yao, <strong>Dan Friedman</strong>, Matthew Hardy, Thomas L. Griffiths
				<br>
				PNAS 2024
				<br>
				<a href="https://www.pnas.org/doi/10.1073/pnas.2322420121">Paper</a> | <a href="https://arxiv.org/abs/2309.13638">arXiv</a> | <a href="https://github.com/tommccoy1/embers-of-autoregression">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2403.03942">
					The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models
				</a>
				<br>
				Adithya Bhaskar, <strong>Dan Friedman</strong>, Danqi Chen
				<br>
				ACL 2024
				<br>
				<a href="https://arxiv.org/abs/2403.03942">Paper</a> | <a href="https://github.com/princeton-nlp/Heuristic-Core">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2312.03656">
					Interpretability Illusions in the Generalization of Simplified Models
				</a>
				<br>
				<strong>Dan Friedman</strong>, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun
				<br>
				ICML 2024
				<br>
				<a href="https://arxiv.org/abs/2312.03656">Paper</a>
			</li>	    
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2306.01128">
					Learning Transformer Programs
				</a>
				<br>
				<strong>Dan Friedman</strong>, Alexander Wettig, Danqi Chen
				<br>
				NeurIPS 2023 <strong>Oral</strong>
				<br>
				<a href="https://arxiv.org/abs/2306.01128">Paper</a> 
				| <a href="https://github.com/princeton-nlp/TransformerPrograms">Code</a>
				| <a href="https://nips.cc/virtual/2023/oral/73853">Talk</a>
				| <a href="https://twimlai.com/podcast/twimlai/learning-transformer-programs/">Interview on TWIML AI podcast</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2305.13299">
					Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations
				</a>
				<br>
				Chenglei Si*, <strong>Dan Friedman</strong>*, Nitish Joshi, Shi Feng, Danqi Chen, He He
				<br>
				ACL 2023
				<br>
				<a href="https://arxiv.org/abs/2305.13299">Paper</a> | <a href="https://github.com/NoviScl/AmbigPrompt">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://openreview.net/forum?id=g97OHbQyk1">
					The Vendi Score: A Diversity Evaluation Metric for Machine Learning
				</a>
				<br>
				<strong>Dan Friedman</strong>, Adji Bousso Dieng
				<br>
				Transactions of Machine Learning Research (TMLR) 2023
				<br>
				<a href="https://openreview.net/forum?id=g97OHbQyk1">Paper</a> | <a href="https://github.com/vertaix/Vendi-Score">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2210.11560">
					Finding Dataset Shortcuts with Grammar Induction
				</a>
				<br>
				<strong>Dan Friedman</strong>, Alexander Wetting, Danqi Chen
				<br>
				EMNLP 2022
				<br>
				<a href="https://arxiv.org/abs/2210.11560">Paper</a> | <a href="https://github.com/princeton-nlp/ShortcutGrammar">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2109.13880">
					Single-dataset Experts for Multi-dataset Question Answering
				</a>
				<br>
				<strong>Dan Friedman</strong>, Ben Dodge, Danqi Chen
				<br>
				EMNLP 2021
				<br>
				<a href="https://arxiv.org/abs/2109.13880">Paper</a> | <a href="https://github.com/princeton-nlp/MADE">Code</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2104.05240">
					Factual Probing is &#91;MASK&#93;: Learning vs. Learning
					to Recall
				</a>
				<br>
				Zexuan Zhong*, <strong>Dan Friedman</strong>*, Danqi Chen
				<br>
				NAACL 2021
				<br>
				<a href="https://arxiv.org/abs/2104.05240">Paper</a> | <a href="https://github.com/princeton-nlp/OptiPrompt">Code</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://www.aclweb.org/anthology/N19-1075.pdf">
					Syntax-aware Neural Semantic Role Labeling with Supertags
				</a>
				<br>
				Jungo Kasai, <strong>Dan Friedman</strong>, Robert Frank,
				Dragomir Radev, Owen Rambow
				<br>
				NAACL 2019
				<br>
				<a href="https://www.aclweb.org/anthology/N19-1075.pdf">Paper</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/1909.01716">
					ScisummNet: A Large Annotated Corpus and Content-Impact
					Models for Scientific Paper Summarization with Citation
					Networks
				</a>
				<br>
				Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R
				Fabbri, Irene Li, <strong>Dan Friedman</strong>, Dragomir R
				Radev
				<br>
				AAAI 2019
				<br>
				<a href="https://arxiv.org/abs/1909.01716">Paper</a>
				 | <a href="https://cs.stanford.edu/~myasu/projects/scisumm_net/">Project page</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://www.aclweb.org/anthology/W17-6213.pdf">
					Linguistically Rich Vector Representations of Supertags for
					TAG Parsing
				</a>
				<br>
				<strong>Dan Friedman</strong><sup>*</sup>, Jungo
				Kasai<sup>*</sup>, R Thomas McCoy<sup>*</sup>, Robert Frank,
				Forrest Davis, Owen Rambow
				<br>
				<em>
					Proceedings of the 13th International Workshop on Tree Adjoining
					Grammars and Related Formalisms
				</em> 2017
				<br>
				<a href="https://www.aclweb.org/anthology/W17-6213.pdf">Paper</a>
			</li>
			<br>
		</ul>
	</body>

</html>
