<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<title>Dan Friedman</title>
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
		<link href="style.css" rel="stylesheet">
	</head>

	<body>
		<h1 style="text-align: center">Dan Friedman</h1>
		<div class="container">
			<div style="flex-basis: 74%">
				<p>
				I'm a fifth-year PhD student in the
				<a href="https://nlp.cs.princeton.edu/">Princeton NLP group</a>,
				working with
				<a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>.
				For the summer and fall of 2023, I was a student researcher at Google Research, where I was advised by
				<a href="https://web.media.mit.edu/~asma_gh/">Asma Ghandeharioun</a> and
				<a href="https://lampinen.github.io/">Andrew Lampinen</a>.
				Before this I was a software engineer at Google and IBM,
				and before that I was an undergraduate at Yale where I worked
				with <a href="https://bobfrank1.github.io/">Bob Frank</a> and
				<a href="http://www.cs.yale.edu/homes/radev/">Dragomir Radev</a>,
				and received a BA in English.
				</p>
				<p style="text-align:center">
				<a href="mailto:dfriedman@princeton.edu">Email</a> &nbsp;/&nbsp;
				<a href="https://scholar.google.com/citations?user=1UMQ_KwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
				<a href="https://twitter.com/danfriedman0">Twitter</a> &nbsp;/&nbsp;
				<a href="https://github.com/danfriedman0/">Github</a>
				</p>
			</div>
			<img src="profile.jpg" width="200" height="200" style="flex-basis: 24%">
		</div>
		<h2>Research Interests</h2>
		<p>
		I'm currently interested in making large, neural language models easier to understand.
		One direction I'm especially interested in is to design models that are
		<a href="https://arxiv.org/abs/2306.01128">inherently interpretable</a>,
		so that we can automatically convert models into formats that are easier to inspect and understand, such as discrete computer programs.
		I'm also interested in approaches that take a more behavioral view, to better characterize the strengths and limitations of large language models
		(<a href="https://arxiv.org/abs/2309.13638">example 1</a>;
		<a href="https://arxiv.org/abs/2305.13299">example 2</a>).
		Some of my more general interests include unsupervised structure learning, formal languages,
		probabilistic models, and inductive bias.
		I'm also interested in applications of NLP to humanities research
		and am involved with the
		<a href="https://cdh.princeton.edu/">
			Princeton Center for Digital Humanities
		</a>.
		</p>
		<h2>Publications and preprints</h2>
		<ul>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2407.10949">
					Representing Rule-based Chatbots with Transformers
				</a>
				<br>
				<strong>Dan Friedman</strong>, Abhishek Panigrahi, Danqi Chen
				<br>
				arXiv 2024
				<br>
				<a href="https://arxiv.org/abs/2407.10949">Paper</a> | <a href="https://github.com/princeton-nlp/ELIZA-Transformer">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2406.16778">
					Finding Transformer Circuits with Edge Pruning
				</a>
				<br>
				Adithya Bhaskar, Alexander Wettig, <strong>Dan Friedman</strong>, Danqi Chen
				<br>
				arXiv 2024
				<br>
				<a href="https://arxiv.org/abs/2406.16778">Paper</a> | <a href="https://github.com/princeton-nlp/Edge-Pruning">Code</a>
			</li>	    
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2403.03942">
					The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models
				</a>
				<br>
				Adithya Bhaskar, <strong>Dan Friedman</strong>, Danqi Chen
				<br>
				ACL 2024
				<br>
				<a href="https://arxiv.org/abs/2403.03942">Paper</a> | <a href="https://github.com/princeton-nlp/Heuristic-Core">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2312.03656">
					Interpretability Illusions in the Generalization of Simplified Models
				</a>
				<br>
				<strong>Dan Friedman</strong>, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun
				<br>
				ICML 2024
				<br>
				<a href="https://arxiv.org/abs/2312.03656">Paper</a>
			</li>	    
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2306.01128">
					Learning Transformer Programs
				</a>
				<br>
				<strong>Dan Friedman</strong>, Alexander Wettig, Danqi Chen
				<br>
				NeurIPS 2023 <strong>Oral</strong>
				<br>
				<a href="https://arxiv.org/abs/2306.01128">Paper</a> 
				| <a href="https://github.com/princeton-nlp/TransformerPrograms">Code</a>
				| <a href="https://nips.cc/virtual/2023/oral/73853">Talk</a>
				| <a href="https://twimlai.com/podcast/twimlai/learning-transformer-programs/">Interview on TWIML AI podcast</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2309.13638">
					Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve
				</a>
				<br>
				R. Thomas McCoy, Shunyu Yao, <strong>Dan Friedman</strong>, Matthew Hardy, Thomas L. Griffiths
				<br>
				arXiv 2023
				<br>
				<a href="https://arxiv.org/abs/2309.13638">Paper</a> | <a href="https://github.com/tommccoy1/embers-of-autoregression">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2305.13299">
					Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations
				</a>
				<br>
				Chenglei Si*, <strong>Dan Friedman</strong>*, Nitish Joshi, Shi Feng, Danqi Chen, He He
				<br>
				ACL 2023
				<br>
				<a href="https://arxiv.org/abs/2305.13299">Paper</a> | <a href="https://github.com/NoviScl/AmbigPrompt">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://openreview.net/forum?id=g97OHbQyk1">
					The Vendi Score: A Diversity Evaluation Metric for Machine Learning
				</a>
				<br>
				<strong>Dan Friedman</strong>, Adji Bousso Dieng
				<br>
				Transactions of Machine Learning Research (TMLR) 2023
				<br>
				<a href="https://openreview.net/forum?id=g97OHbQyk1">Paper</a> | <a href="https://github.com/vertaix/Vendi-Score">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2210.11560">
					Finding Dataset Shortcuts with Grammar Induction
				</a>
				<br>
				<strong>Dan Friedman</strong>, Alexander Wetting, Danqi Chen
				<br>
				EMNLP 2022
				<br>
				<a href="https://arxiv.org/abs/2210.11560">Paper</a> | <a href="https://github.com/princeton-nlp/ShortcutGrammar">Code</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2109.13880">
					Single-dataset Experts for Multi-dataset Question Answering
				</a>
				<br>
				<strong>Dan Friedman</strong>, Ben Dodge, Danqi Chen
				<br>
				EMNLP 2021
				<br>
				<a href="https://arxiv.org/abs/2109.13880">Paper</a> | <a href="https://github.com/princeton-nlp/MADE">Code</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/2104.05240">
					Factual Probing is &#91;MASK&#93;: Learning vs. Learning
					to Recall
				</a>
				<br>
				Zexuan Zhong*, <strong>Dan Friedman</strong>*, Danqi Chen
				<br>
				NAACL 2021
				<br>
				<a href="https://arxiv.org/abs/2104.05240">Paper</a> | <a href="https://github.com/princeton-nlp/OptiPrompt">Code</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://www.aclweb.org/anthology/N19-1075.pdf">
					Syntax-aware Neural Semantic Role Labeling with Supertags
				</a>
				<br>
				Jungo Kasai, <strong>Dan Friedman</strong>, Robert Frank,
				Dragomir Radev, Owen Rambow
				<br>
				NAACL 2019
				<br>
				<a href="https://www.aclweb.org/anthology/N19-1075.pdf">Paper</a>
			</li> 
			<br>
			<li>
				<a class="paper" href="https://arxiv.org/abs/1909.01716">
					ScisummNet: A Large Annotated Corpus and Content-Impact
					Models for Scientific Paper Summarization with Citation
					Networks
				</a>
				<br>
				Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R
				Fabbri, Irene Li, <strong>Dan Friedman</strong>, Dragomir R
				Radev
				<br>
				AAAI 2019
				<br>
				<a href="https://arxiv.org/abs/1909.01716">Paper</a>
				 | <a href="https://cs.stanford.edu/~myasu/projects/scisumm_net/">Project page</a>
			</li>
			<br>
			<li>
				<a class="paper" href="https://www.aclweb.org/anthology/W17-6213.pdf">
					Linguistically Rich Vector Representations of Supertags for
					TAG Parsing
				</a>
				<br>
				<strong>Dan Friedman</strong><sup>*</sup>, Jungo
				Kasai<sup>*</sup>, R Thomas McCoy<sup>*</sup>, Robert Frank,
				Forrest Davis, Owen Rambow
				<br>
				<em>
					Proceedings of the 13th International Workshop on Tree Adjoining
					Grammars and Related Formalisms
				</em> 2017
				<br>
				<a href="https://www.aclweb.org/anthology/W17-6213.pdf">Paper</a>
			</li>
			<br>
		</ul>
	</body>

</html>
